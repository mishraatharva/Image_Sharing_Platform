{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### first try:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "easyocr 1.7.1 requires ninja, which is not installed.\n",
      "easyocr 1.7.1 requires pyclipper, which is not installed.\n",
      "easyocr 1.7.1 requires python-bidi, which is not installed.\n",
      "easyocr 1.7.1 requires scikit-image, which is not installed.\n",
      "easyocr 1.7.1 requires scipy, which is not installed.\n",
      "easyocr 1.7.1 requires Shapely, which is not installed.\n",
      "easyocr 1.7.1 requires torch, which is not installed.\n",
      "easyocr 1.7.1 requires torchvision>=0.5, which is not installed.\n"
     ]
    }
   ],
   "source": [
    "!pip install -q datasets\n",
    "# https://youtu.be/3bPhDUSAUYI?si=eTexaaA3cJMDIYcq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: datasets\n",
      "Version: 3.1.0\n",
      "Summary: HuggingFace community-driven open-source library of datasets\n",
      "Home-page: https://github.com/huggingface/datasets\n",
      "Author: HuggingFace Inc.\n",
      "Author-email: thomas@huggingface.co\n",
      "License: Apache 2.0\n",
      "Location: u:\\nlp_project\\image_sharing_plateform\\venv\\lib\\site-packages\n",
      "Requires: aiohttp, dill, filelock, fsspec, huggingface-hub, multiprocess, numpy, packaging, pandas, pyarrow, pyyaml, requests, tqdm, xxhash\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "!pip show datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "captions = [{\"file_name\": \"ronaldo.jpeg\", \"text\": \"Ronaldo with Portugal at the 2018 World Cup\"},\n",
    "{\"file_name\": \"messi.jpeg\", \"text\": \"Messi with Argentina at the 2022 FIFA World Cup\"},\n",
    "{\"file_name\": \"zidane.jpeg\", \"text\": \"Zin√©dine Zidane pendant la finale de la Coupe du monde 2006.\"},\n",
    "{\"file_name\": \"maradona.jpeg\", \"text\": \"Maradona after winning the 1986 FIFA World Cup with Argentina\"},\n",
    "{\"file_name\": \"ronaldo_.jpeg\", \"text\": \"Ronaldo won La Liga in his first season and received the Pichichi Trophy in his second.\"},\n",
    "{\"file_name\": \"pirlo.jpeg\", \"text\": \"Pirlo with Juventus in 2014\"},]\n",
    "\n",
    "# path to the folder containing the images\n",
    "root = r\"U:\\nlp_project\\Image_Sharing_Plateform\\data\\temp\"\n",
    "\n",
    "# add metadata.jsonl file to this folder\n",
    "with open(root + \"metadata.jsonl\", 'w') as f:\n",
    "    for item in captions:\n",
    "        f.write(json.dumps(item) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 6 examples [00:00, 851.49 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset \n",
    "\n",
    "dataset = load_dataset(r\"U:\\nlp_project\\Image_Sharing_Plateform\\data\\temp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds = dataset[\"train\"].train_test_split(test_size=0.1)\n",
    "# train_ds = ds[\"train\"]\n",
    "# test_ds = ds[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from textwrap import wrap\n",
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "\n",
    "\n",
    "# def plot_images(images, captions):\n",
    "#     plt.figure(figsize=(20, 20))\n",
    "#     for i in range(len(images)):\n",
    "#         ax = plt.subplot(1, len(images), i + 1)\n",
    "#         caption = captions[i]\n",
    "#         caption = \"\\n\".join(wrap(caption, 12))\n",
    "#         plt.title(caption)\n",
    "#         plt.imshow(images[i])\n",
    "#         plt.axis(\"off\")\n",
    "\n",
    "\n",
    "# sample_images_to_visualize = [np.array(train_ds[i][\"image\"]) for i in range(5)]\n",
    "# sample_captions = [train_ds[i][\"text\"] for i in range(5)]\n",
    "# plot_images(sample_images_to_visualize, sample_captions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### second try:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
