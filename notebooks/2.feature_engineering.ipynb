{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "from ensure import ensure_annotations\n",
    "from pathlib import Path\n",
    "from typing import Any\n",
    "import yaml\n",
    "from types import SimpleNamespace\n",
    "import string\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import tensorflow as tf\n",
    "from keras.layers import TextVectorization\n",
    "from pickle import dump, load\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_to_namespace(d):\n",
    "    \"\"\"Recursively converts a dictionary into a SimpleNamespace object.\"\"\"\n",
    "    if isinstance(d, dict):\n",
    "        return SimpleNamespace(**{k: dict_to_namespace(v) for k, v in d.items()})\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def create_directories(path_to_directories: list, verbose=True):\n",
    "    for path in path_to_directories:\n",
    "        os.makedirs(path, exist_ok=True)\n",
    "\n",
    "def read_yaml(path_to_yaml: Path):\n",
    "    try:\n",
    "        with open(path_to_yaml, \"r\") as yaml_file:\n",
    "            content = yaml.safe_load(yaml_file)\n",
    "            return dict_to_namespace(content)  # Convert dict to namespace\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {path_to_yaml}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_doc(file_name):\n",
    "    \"\"\"\n",
    "    Loads the document file and reads its contents into a string.\n",
    "    \"\"\"\n",
    "    # a = \"U:/nlp_project/Image_Sharing_Plateform/data/processed\"\n",
    "    # file_name = os.path.join()  # Use raw string\n",
    "    file_path = Path(file_name)  # Convert to Path object\n",
    "\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:  # Use 'with open' to avoid manual close\n",
    "        text = file.read()\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = load_doc(r\"U:\\nlp_project\\Image_Sharing_Plateform\\data\\processed\\training_data.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.image_sharing_plateform.constants import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class TrainTestSplit():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def train_val_split(caption_data, train_size=0.8, shuffle=True):\n",
    "\n",
    "       # 1. Get the list of all image names\n",
    "        all_images = list(caption_data.keys())\n",
    "\n",
    "        # 2. Shuffle if necessary\n",
    "        if shuffle:\n",
    "            np.random.shuffle(all_images)\n",
    "\n",
    "        # 3. Split into training and validation sets\n",
    "        train_size = int(len(caption_data) * train_size)\n",
    "\n",
    "        training_data = {\n",
    "            img_name: caption_data[img_name] for img_name in all_images[:train_size]\n",
    "        }\n",
    "        validation_data = {\n",
    "            img_name: caption_data[img_name] for img_name in all_images[train_size:]\n",
    "        }\n",
    "\n",
    "        # 4. Return the splits\n",
    "        return training_data, validation_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8092*0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8092-6473"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class DataTransformationConfig:\n",
    "    original_image_data_path: Path\n",
    "    original_caption_data_path: Path\n",
    "    preprocessed_data_path : Path\n",
    "    extracted_features_path : Path\n",
    "    training_data : Path\n",
    "    validation_data : Path\n",
    "    vectorizer_path : Path\n",
    "    SEQ_LENGTH : int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataTransformationConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath = CONFIG_FILE_PATH):\n",
    "        self.config = read_yaml(config_filepath)\n",
    "\n",
    "    \n",
    "    def get_data_transformation_config(self) -> DataTransformationConfig:\n",
    "        config = self.config.data_transformation\n",
    "\n",
    "        create_directories([config.training_data, config.validation_data, config.vectorizer_path])\n",
    "\n",
    "        data_transformation_config = DataTransformationConfig(\n",
    "            original_image_data_path = config.original_image_data_path,\n",
    "\n",
    "            original_caption_data_path = config.original_caption_data_path,\n",
    "            \n",
    "            preprocessed_data_path = config.preprocessed_data_path,\n",
    "\n",
    "            extracted_features_path = config.extracted_features_path,\n",
    "\n",
    "            training_data = config.training_data,\n",
    "            \n",
    "            validation_data = config.validation_data,\n",
    "            \n",
    "            vectorizer_path = config.vectorizer_path,\n",
    "\n",
    "            SEQ_LENGTH = config.SEQ_LENGTH\n",
    "            \n",
    "        )\n",
    "        return data_transformation_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataTransformation:\n",
    "    def __init__(self, config: DataTransformationConfig):\n",
    "        self.config = config\n",
    "\n",
    "    def all_img_captions(self):\n",
    "        \"\"\"This function will take original caption data and return:\n",
    "        'descriptions': a dictionary containing key value pair of image and list of all captions of each image. In original data image and there caption is repeating.\n",
    "        \"\"\"\n",
    "        path = self.config.original_caption_data_path + \"/\" + \"Flickr8k.token.txt\" \n",
    "\n",
    "        file = load_doc(Path(path))\n",
    "        captions = file.split('\\n')\n",
    "        descriptions ={}\n",
    "        text_data = []\n",
    "        for caption in captions:\n",
    "            if '\\t' in caption:\n",
    "                img, caption = caption.split('\\t')\n",
    "                if img[:-2] not in descriptions:\n",
    "                    descriptions[img[:-2]] = [ caption ]\n",
    "                else:\n",
    "                    descriptions[img[:-2]].append(caption)\n",
    "                # text_data.append(caption)\n",
    "        return descriptions\n",
    "    \n",
    "    \n",
    "    def cleaning_text(self,captions):\n",
    "        table = str.maketrans('','',string.punctuation)\n",
    "        for img,caps in captions.items():\n",
    "            for i,img_caption in enumerate(caps):\n",
    "                img_caption.replace(\"-\",\" \")\n",
    "                desc = img_caption.split()\n",
    "                \n",
    "                #converts to lowercase\n",
    "                desc = [word.lower() for word in desc]\n",
    "                \n",
    "                #remove punctuation from each token\n",
    "                desc = [word.translate(table) for word in desc]\n",
    "                \n",
    "                #remove hanging 's and a \n",
    "                desc = [word for word in desc if(len(word)>1)]\n",
    "                \n",
    "                #remove tokens with numbers in them\n",
    "                desc = [word for word in desc if(word.isalpha())]\n",
    "                \n",
    "                #convert back to string\n",
    "                img_caption = ' '.join(desc)\n",
    "                img_caption = '<start> ' + \" \".join(desc) + ' <end>'\n",
    "                captions[img][i]= img_caption\n",
    "        return captions\n",
    "    \n",
    "    def save_descriptions(self,descriptions, filename):\n",
    "        lines = list()\n",
    "        for key, desc_list in descriptions.items():\n",
    "            for desc in desc_list:\n",
    "                lines.append(key + '\\t' + desc )\n",
    "        data = \"\\n\".join(lines)\n",
    "        \n",
    "        cleaned_data_path = self.config.preprocessed_data_path + \"/\" +  filename\n",
    "        \n",
    "        with open(cleaned_data_path, \"w\", encoding=\"utf-8\") as file:\n",
    "            file.write(data)\n",
    "\n",
    "    \n",
    "    def create_photos(self,filename):\n",
    "        file_path = self.config.preprocessed_data_path + \"/\"+ filename\n",
    "        file = load_doc(file_path)\n",
    "        train_images = []\n",
    "        texts = file.split(\"\\n\")[:-1]\n",
    "        for text in texts:\n",
    "            text = text.split(\"\\t\")\n",
    "            train_images.append(text[0])\n",
    "        return set(train_images)\n",
    "    \n",
    "    def create_features(self,photos,filename):\n",
    "        #loading all features\n",
    "        file_path = self.config.extracted_features_path + \"/\"+ filename\n",
    "        train_data_features = {}\n",
    "        all_features = load(open(file_path,\"rb\"))\n",
    "        #selecting only needed features\n",
    "        for ph in photos:\n",
    "            if ph in all_features.keys():\n",
    "                train_data_features[ph] = all_features[ph]\n",
    "            else:\n",
    "                train_data_features[ph] = []\n",
    "        return train_data_features\n",
    "    \n",
    "\n",
    "    def clean_final_data(self,data_captions, data_images, data_features):\n",
    "        invalid_ids = []  # Store invalid image IDs\n",
    "\n",
    "        for img_id, _ in data_captions.items():\n",
    "            image_feature = data_features[img_id]  # Extract image feature vector\n",
    "            image_feature = np.array(image_feature)\n",
    "\n",
    "            if image_feature.shape != (512,):  # Check if the shape is incorrect\n",
    "                # print(img_id, image_feature.shape)\n",
    "                invalid_ids.append(img_id)  # Collect invalid IDs\n",
    "\n",
    "        # Delete all invalid IDs **after** iteration\n",
    "        for img_id in invalid_ids:\n",
    "            del data_captions[img_id] \n",
    "            del data_features[img_id]\n",
    "            data_images.discard(img_id)\n",
    "\n",
    "        return data_captions, data_features, data_images\n",
    "    \n",
    "    def get_vectorizer(self, train_data_captions):\n",
    "\n",
    "        \"\"\"Here data required is in list format. So converting data in required format only i.e list\"\"\"\n",
    "        all_desc = []\n",
    "\n",
    "        for key in train_data_captions.keys():\n",
    "            all_desc = all_desc + train_data_captions[key]\n",
    "        \n",
    "        vectorizer = tf.keras.layers.TextVectorization(\n",
    "                            max_tokens=7151,\n",
    "                            output_mode=\"int\", \n",
    "                            output_sequence_length=self.config.SEQ_LENGTH\n",
    "                    )\n",
    "        # print(all_desc)\n",
    "        vectorizer.adapt(all_desc)\n",
    "        \n",
    "        return vectorizer\n",
    "    \n",
    "\n",
    "    def vectorize_data(self,vectorizer,data):\n",
    "        tokenized_data = {img_id: vectorizer([f\"<start> {cap} <end>\"]) for img_id, caps in data.items() for cap in caps}\n",
    "        return tokenized_data\n",
    "        \n",
    "    \n",
    "    def save_training_validation_data(self,data_caption, data_image, data_feature, data):\n",
    "        \n",
    "        if data == \"train\":\n",
    "            path = self.config.training_data + \"/\" + \"train_data.pkl\"\n",
    "        else:\n",
    "            path = self.config.validation_data + \"/\" + \"validation_data.pkl\"\n",
    "\n",
    "        with open(path, \"wb\") as file:\n",
    "            pickle.dump({\"caption_data\": data_caption, \"image_data\": data_image, \"feature_data\": data_feature}, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "INFO:tensorflow:Assets written to: data/processed/vectorizer/vectorizer\\assets\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    \"\"\"\n",
    "    STAGE1:\n",
    "    step1: Create ConfigurationManager() object.\n",
    "    step2: get all configuration and values config.yaml file.\n",
    "    step3: create a dictionary 'descriptions' containing image is a key : all caption in a list as a value.\n",
    "    step4: clean all like :=  'converts to lowercase', 'remove punctuation from each token', 'remove hanging 's and a'.\n",
    "    step5: split full data in train and validation data. i.e 'training_data' and 'validation_data'.\n",
    "    step5: save this 'training_data' and 'validation_data' in .txt format in desired location.\n",
    "    \n",
    "    STAGE2:\n",
    "    step1: read training_data.txt and validation_data.txt.\n",
    "    step2: create a list of all image in image data and create a dictionary of image_id as key and his vgg generated feature of shape (512,).\n",
    "    step3:  create vectorizer on train_data\n",
    "    step4: vectorize 'train_data' and create 'train_data_caption' which in vectorized data.\n",
    "    step5: repeat step2, step3, step4 for validation data as well.\n",
    "    step6: save 'train_data_captions, train_data_images, train_data_features'.\n",
    "    step7: save 'validation_data_captions, validation_data_images, validation_data_features'.\n",
    "    step8: save vectorizer object for further use.\n",
    "    \"\"\"\n",
    "    config = DataTransformationConfigurationManager()\n",
    "    # train_split = TrainTestSplit()\n",
    "    \n",
    "    data_transformation_config = config.get_data_transformation_config()\n",
    "    \n",
    "    data_transformation = DataTransformation(config=data_transformation_config)\n",
    "    \n",
    "    descriptions = data_transformation.all_img_captions()\n",
    "\n",
    "    full_data = data_transformation.cleaning_text(descriptions)\n",
    "\n",
    "    training_data, validation_data = TrainTestSplit.train_val_split(full_data)\n",
    "    \n",
    "    data_transformation.save_descriptions(training_data,\"training_data.txt\")\n",
    "    data_transformation.save_descriptions(validation_data,\"validation_data.txt\")\n",
    "\n",
    "    # train_data_captions = data_transformation.create_captions(\"training_data.txt\")\n",
    "    train_data_images = data_transformation.create_photos(\"training_data.txt\")\n",
    "    train_data_features = data_transformation.create_features(train_data_images, \"extracted_features.p\")\n",
    "    train_data_captions, train_data_images, train_data_features = data_transformation.clean_final_data(training_data, train_data_images, train_data_features)\n",
    "    \n",
    "    \n",
    "    vectorizer = data_transformation.get_vectorizer(training_data)\n",
    "    train_data_captions = data_transformation.vectorize_data(vectorizer,training_data)\n",
    "    \n",
    "\n",
    "    # validation_data_captions = data_transformation.create_captions(r\"validation_data.txt\")\n",
    "    validation_data_images = data_transformation.create_photos(r\"validation_data.txt\")\n",
    "    validation_data_features = data_transformation.create_features(validation_data_images ,\"extracted_features.p\")\n",
    "    validation_data_captions, validation_data_images, validation_data_features= data_transformation.clean_final_data(validation_data, validation_data_images, validation_data_features)\n",
    "\n",
    "    validation_data_captions = data_transformation.vectorize_data(vectorizer,validation_data_captions)\n",
    "\n",
    "    data_transformation.save_training_validation_data(train_data_captions, train_data_images, train_data_features, \"train\")\n",
    "    data_transformation.save_training_validation_data(validation_data_captions, validation_data_images, validation_data_features, \"validation\")\n",
    "\n",
    "    vectorizer = tf.keras.models.Sequential([vectorizer])\n",
    "    # vectorizer.save(r\"U:\\nlp_project\\Image_Sharing_Plateform\\data\\processed\\vectorizer\")\n",
    "\n",
    "    vectorizer_path = data_transformation_config.vectorizer_path + \"/\" + \"vectorizer\"\n",
    "    vectorizer.save(vectorizer_path)\n",
    "\n",
    "except Exception as e:\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6472, 6472, 6472)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data_images),len(train_data_features), len(train_data_captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1619, 6472, 6472)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(validation_data_images),len(training_data),len(train_data_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
